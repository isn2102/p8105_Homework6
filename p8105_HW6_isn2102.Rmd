---
title: "p8105_hw6_isn2102.Rmd"
author: "Isabel Nelson"
date: "11/22/2020"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(patchwork)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
set.seed(1)
```

### Problem 1
```{r, message = FALSE}
homicide_df <- 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age), 
    resolved = case_when(
      disposition == "Closed without arrest" ~ 0, 
      disposition == "Open/No arrest" ~ 0, 
      disposition == "Closed by arrest" ~ 1,
    )
  ) %>% 
  filter(
    city_state != "Tulsa, AL", 
    victim_race %in% c("White", "Black")) %>% 
  select(city_state, resolved, victim_age, victim_race, victim_sex)
```

Start with one city, fit a linear model to relate resolution to these three variables. 
```{r}
baltimore_df <-
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")

glm(resolved ~ victim_age + victim_race + victim_sex, 
    data = baltimore_df, 
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate), 
    CI_lower = exp(estimate - 1.96 * std.error), 
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
```

Now we will do this with iteration to look at every city. 
```{r}
model_results_df <-
homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = 
      map(.x = data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())), 
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate), 
    CI_lower = exp(estimate - 1.96 * std.error), 
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI"))
```


Make a plot: 
```{r}
model_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


### Problem 2 

Import and clean data: 
```{r, message = FALSE}
birthweight_df <- 
  read_csv("./data/birthweight.csv") %>% 
  mutate(
    babysex = as.factor(babysex), 
    frace = as.factor(frace), 
    malform = as.factor(malform), 
    mrace = as.factor(mrace)
  ) 
```

Check for missing variables: based on the below function to find NAs, there are none for any variable.
```{r}
find_na <-
  birthweight_df %>% 
  map_df(~sum(is.na(.)))

find_na
```

**Model building process:**  

1. Select a subset of variables that are expected to be related to birthweight based on theoretical knowledge: bhead, blength, delwt, fincome, gaweeks, momage, mrace, pnumlbw, pnumsga, ppbmi, smoken, wtgain  

2. Look at plots to see the relationship between each possible predictor and the outcome. Choose predictors that have a linear relationship with the outcome to move forward.   
```{r}
bhead_plot <-
birthweight_df %>% 
  ggplot(aes(x = bhead, y = bwt)) + 
  geom_point()
blength_plot <-
birthweight_df %>% 
  ggplot(aes(x = blength, y = bwt)) + 
  geom_point()
delwt_plot <-
birthweight_df %>% 
  ggplot(aes(x = delwt, y = bwt)) + 
  geom_point()
fincome_plot <-
birthweight_df %>% 
  ggplot(aes(x = fincome, y = bwt)) + 
  geom_point()
gaweeks_plot <-
birthweight_df %>% 
  ggplot(aes(x = gaweeks, y = bwt)) + 
  geom_point()
momage_plot <-
birthweight_df %>% 
  ggplot(aes(x = momage, y = bwt)) + 
  geom_point()
mrace_plot <-
birthweight_df %>% 
  ggplot(aes(x = mrace, y = bwt)) + 
  geom_point()
pnumlbw_plot <-
birthweight_df %>% 
  ggplot(aes(x = pnumlbw, y = bwt)) + 
  geom_point()
pnumsga_plot <-
birthweight_df %>% 
  ggplot(aes(x = pnumsga, y = bwt)) + 
  geom_point()
ppbmi_plot <-
birthweight_df %>% 
  ggplot(aes(x = ppbmi, y = bwt)) + 
  geom_point()
smoken_plot <-
birthweight_df %>% 
  ggplot(aes(x = smoken, y = bwt)) + 
  geom_point()
wtgain_plot <-
birthweight_df %>% 
  ggplot(aes(x = wtgain, y = bwt)) + 
  geom_point()

(bhead_plot + blength_plot + delwt_plot + fincome_plot + gaweeks_plot + momage_plot + mrace_plot + pnumlbw_plot + pnumsga_plot + ppbmi_plot + smoken_plot + wtgain_plot + plot_layout(ncol = 4))
```

From these plots we see that pnumlbw and pnumsga only have one value each for all observations (zero), so we will remove those two. Additionally, it looks like wtgain is not linear so we will remove that as well. Mrace can be coded as dummy variables so we will keep that in the model for now.  


3. Fit a bivariate linear model for each variable, and choose variables that seem like they may have a substantial relationship.
```{r}
bhead_biv <- lm(bwt ~ bhead, data = birthweight_df)
broom::tidy(bhead_biv) %>% knitr::kable(digits = 5)

blength_biv <- lm(bwt ~ blength, data = birthweight_df)
broom::tidy(blength_biv) %>% knitr::kable(digits = 5)

delwt_biv <- lm(bwt ~ delwt, data = birthweight_df)
broom::tidy(delwt_biv) %>% knitr::kable(digits = 5)

fincome_biv <- lm(bwt ~ fincome, data = birthweight_df)
broom::tidy(fincome_biv) %>% knitr::kable(digits = 5)

gaweeks_biv <- lm(bwt ~ gaweeks, data = birthweight_df)
broom::tidy(gaweeks_biv) %>% knitr::kable(digits = 5)

momage_biv <- lm(bwt ~ momage, data = birthweight_df)
broom::tidy(momage_biv) %>% knitr::kable(digits = 5)

mrace_biv <- lm(bwt ~ mrace, data = birthweight_df)
broom::tidy(mrace_biv) %>% knitr::kable(digits = 5)

ppbmi_biv <- lm(bwt ~ ppbmi, data = birthweight_df)
broom::tidy(ppbmi_biv) %>% knitr::kable(digits = 5)

smoken_biv <- lm(bwt ~ smoken, data = birthweight_df)
broom::tidy(smoken_biv) %>% knitr::kable(digits = 5)
```

From this exploration we can see that all variables tested had p-values that were very small (most show up as zero) so I will keep all of them. 

4. Fit a preliminary model with all these predictors.  
```{r}
prelim_model <- lm(bwt ~ bhead + blength + delwt + fincome + gaweeks + momage + mrace + ppbmi + smoken, data = birthweight_df)
```

5. Look at the t test statistic for each variable to see significant ones. Also remove predictor from model and if the other parameter estimates change, keep variable in the model. Remove variables that do not change other estimates much when removing them, or with insignificant p-values.  
```{r}
summary(prelim_model) 

prelim_model2 <- lm(bwt ~ bhead + blength + delwt + gaweeks + mrace + ppbmi + smoken, data = birthweight_df)
summary(prelim_model2)

prelim_model3 <- lm(bwt ~ bhead + blength + delwt + gaweeks + ppbmi + mrace, data = birthweight_df)
summary(prelim_model3)

prelim_model4 <- lm(bwt ~ bhead + blength + delwt + gaweeks + mrace, data = birthweight_df)
summary(prelim_model4)

prelim_model5 <- lm(bwt ~ bhead + blength + gaweeks + mrace, data = birthweight_df)
summary(prelim_model5)
```

From this regression output we see that fincome and momage are not significant anymore so we will remove those. I removed smoken, delwt, and ppbmi variables and the other coefficients didn't change much, and the R^2 is still very high (0.70 compared to 0.72 with the "full" model) so I will keep those variables out of model. My final model (prelim_model5) includes bhead, blength, gaweeks, and mrace.

Plot model residuals against fitted values:
```{r}
birthweight_df %>% 
add_residuals(prelim_model5) %>% 
add_predictions(prelim_model5) %>% 
ggplot(aes(x = pred, y = resid)) + 
geom_point()

birthweight_df %>% 
  modelr::add_residuals(prelim_model5) %>% 
  ggplot(aes(x = resid)) +
  geom_density()
  
```

**Interpretation**  
From the density plot it seems like the residuals are normally distributed which indicates the model fits well. I also looked at residuals plotted against predicted values. We see most of the residual points are centered around zero and have generally even spread, also indicating a pretty good fit. There are a few dots that appear to be outliers and are quite far from zero, indicating that the model may not fit as well as it could. There is no apparent pattern that I can see. 

Fit two additional models: 
```{r}
model_blength_gaweeks <- lm(bwt ~ blength + gaweeks, data = birthweight_df)
summary(model_blength_gaweeks)
model_interactions <- lm(bwt ~ bhead * blength * babysex, data = birthweight_df)
summary(model_interactions)
```

Compare the three models by using a cross-validation approach. First create a set of 100 samples with crossv_mc, then convert to tibbles. Map each model across each sample dataframe, then calculate RMSE based on each fitted model and the testing portion of each sample from the original dataset. 
```{r}
cv_bw_df <- 
  crossv_mc(birthweight_df, 100)

cv_bw_df <-  
  cv_bw_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_bw_df <- 
  cv_bw_df %>% 
  mutate(
    interactions_mod = map(.x = train, ~lm(bwt ~ bhead * blength * babysex, data = .x)),
    blength_gaweeks_mod = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    prelim5_mod = map(.x = train, ~lm(bwt ~ bhead + blength + gaweeks + mrace, data = .x))
  ) %>% 
  mutate(
    rmse_interactions = map2_dbl(.x = interactions_mod, .y = test, ~rmse(model = .x, data = .y)),
    rmse_blength_gaweeks = map2_dbl(.x = blength_gaweeks_mod, .y = test, ~rmse(model = .x, data = .y)),
    rmse_prelim5 = map2_dbl(.x = prelim5_mod, .y = test, ~rmse(model = .x, data = .y))
  )
```

Assess RMSE to see which model is best. Pivot data to tidy, visualize distribution, and calculate the mean RMSE.
```{r}
cv_bw_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()

cv_bw_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  group_by(model) %>% 
  summarize(avg_rmse = mean(rmse)) %>% 
  knitr::kable(digits = 5)
  
```

Because the smallest distribution of RMSE is for the prelim5 model which I initially fit, and because we can see that prelim5 also has the smallest average RMSE, we can conclude that prelim5 predicts birthweight the best in our test data.


### Problem 3  

Create data and clean: 
```{r, message = FALSE}
weather_df <- 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Use 5000 bootstrap samples and produce an estimate for r^2 and log(beta0xbeta1) for each bootstrap sample. First create a helper function to create log(beta0xbeta1) from the broom::tidy results. Then map my linear model with tmax and tmin, broom:glance, and my function to each bootstrap sample. Unnest results and select the ones I want.

```{r}
beta_extract = function(ls) {
  broom::tidy(ls) %>% 
  select(term, estimate) %>% 
    pivot_wider(names_from = term, values_from = estimate) %>% 
    janitor::clean_names() %>% 
    mutate(
      log_betas = log(intercept*tmin))
}
```

```{r}
bs_weather_df <-
weather_df %>% 
  bootstrap(5000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)), 
    b_results = map(models, beta_extract), 
    r_results = map(models, broom::glance)
  ) %>% 
  select(strap_number, b_results, r_results) %>% 
  unnest(b_results, r_results) %>% 
  select(strap_number, r.squared, log_betas)
```

Plot the distribution of the estimates.
```{r}
hist_r.squared <-
bs_weather_df %>% 
  ggplot(aes(x = r.squared)) +
  geom_density(fill = "mediumpurple", color = "mediumpurple", alpha = 0.8) +
  labs(
    title = "Density plot of r squared values"
  )

hist_log_betas <-
  bs_weather_df %>% 
  ggplot(aes(x = log_betas)) +
  geom_density(fill = "turquoise3",color = "turquoise3", alpha = 0.8) +
  labs(
    title = "Density plot of log(beta0*beta1) values"
  )

hist_r.squared + hist_log_betas
```

The distribution of r.squared is bell-shaped but is skewed more to the left than a normal curve. It is centered around 0.91. The distribution of log(beta0*beta1) is more normally shaped and centered around 2.01. 


Find the 95% confidence intervals for the estimates. 
```{r}
bs_weather_df %>% 
  pivot_longer(
    r.squared:log_betas, 
    names_to = "Estimate", 
    values_to = "Value"
  ) %>% 
  group_by(Estimate) %>% 
  summarize(
    ci_lower = quantile(Value, 0.025),
    ci_upper = quantile(Value, 0.975)
  ) %>% 
  knitr::kable(digits = 5)
```

The 95% confidence interval for r^2 is (1.965, 2.059) and the 95% confidence interval for log(beta0*beta1) is (0.893, 0.927). 



